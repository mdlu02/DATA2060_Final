{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA 2060 Final Notebook**\n",
    "\n",
    "blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    '''\n",
    "    Apply softmax to an array\n",
    "    @params:\n",
    "        x: the original array\n",
    "    @return:\n",
    "        an array with softmax applied elementwise.\n",
    "    '''\n",
    "    inner = np.array(x - np.max(x))\n",
    "    e = np.exp(inner.astype(np.float64))\n",
    "    return (e + 1e-6) / (np.sum(e) + 1e-6)\n",
    "\n",
    "class LogisticRegression:\n",
    "    '''\n",
    "    Multiclass Logistic Regression that learns weights using \n",
    "    stochastic gradient descent.\n",
    "    '''\n",
    "    def __init__(self, n_features, n_classes, batch_size, conv_threshold):\n",
    "        '''\n",
    "        Initializes a LogisticRegression classifer.\n",
    "        @attrs:\n",
    "            n_features: the number of features in the classification problem\n",
    "            n_classes: the number of classes in the classification problem\n",
    "            weights: The weights of the Logistic Regression model\n",
    "            alpha: The learning rate used in stochastic gradient descent\n",
    "        '''\n",
    "        self.n_classes = n_classes\n",
    "        self.n_features = n_features\n",
    "        self.weights = np.zeros((n_classes, n_features))  # An extra row added for the bias\n",
    "        self.alpha = 0.03  # DO NOT TUNE THIS PARAMETER\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_threshold = conv_threshold\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        '''\n",
    "        Trains the model using stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            num_epochs: integer representing the number of epochs taken to reach convergence\n",
    "        '''\n",
    "\n",
    "        converge = False\n",
    "        num_epochs = 0\n",
    "        n = len(X)\n",
    "        b = self.batch_size\n",
    "        while not converge:\n",
    "            num_epochs += 1\n",
    "\n",
    "            # shuffle training examples using indicies\n",
    "            p = np.random.permutation(n)\n",
    "            X_ = X[p]\n",
    "            Y_ = Y[p]\n",
    "\n",
    "            # calculate last epoch loss\n",
    "            e_last = self.loss(X_, Y_)\n",
    "\n",
    "            # iterate through each batch, updating weights for each batch\n",
    "            for i in range(int(np.ceil(n/b))):\n",
    "                # get examples and labels in current batch\n",
    "                x_batch = X_[i*b : (i+1)*b]\n",
    "                y_batch = Y_[i*b : (i+1)*b]\n",
    "                n_batch = len(x_batch)\n",
    "\n",
    "                # initialize Lw as a matrix of 0s\n",
    "                Lw = np.zeros((self.n_classes, self.n_features))  # An extra row added for the bias\n",
    "\n",
    "                # for each data point in batch\n",
    "                for d in range(n_batch):\n",
    "                    x = x_batch[d]\n",
    "                    y = y_batch[d]\n",
    "\n",
    "                    probabilities = self.getSigmoidProbability(x)\n",
    "\n",
    "                    for j in range(self.n_classes):\n",
    "                        # calculate partial derivative of loss with respect to single row in weights matrix\n",
    "                        h_w_x = probabilities[j] # probabilitiy at j\n",
    "\n",
    "                        Lw[j,:] = Lw[j,:] + (h_w_x-1) * x\n",
    "                    else:\n",
    "                        Lw[j,:] = Lw[j,:] + h_w_x * x\n",
    "\n",
    "                # update weights\n",
    "                self.weights -= (self.alpha * Lw / n_batch)\n",
    "\n",
    "            # calculate current epoch loss\n",
    "            e_current = self.loss(X_,Y_)\n",
    "\n",
    "            # check convergence\n",
    "            if np.abs(e_current - e_last) < self.conv_threshold:\n",
    "                converge = True\n",
    "\n",
    "        print(\"Finished training with \", num_epochs, \"epochs.\")\n",
    "        return num_epochs\n",
    "\n",
    "    def loss(self, X, Y):\n",
    "        '''\n",
    "        Returns the total log loss on some dataset (X, Y), divided by the number of examples.\n",
    "        @params:\n",
    "            X: 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            A float number which is the average loss of the model on the dataset\n",
    "        '''   \n",
    "        # calculate loss for each example (x,y), then average them\n",
    "            \n",
    "        total_loss = 0\n",
    "        n = len(Y)\n",
    "        predictions = self.predict(X)\n",
    "\n",
    "        for i in range(n):\n",
    "            loss = 0\n",
    "            probabilities = self.getSoftmaxProbability(X[i])\n",
    "\n",
    "            for j in range(self.n_classes):\n",
    "                if Y[i] == j: \n",
    "                    # calculate our probability that x in j based on weights:\n",
    "                    h_w_x = probabilities[j]\n",
    "                    if h_w_x > 0: loss += np.log(h_w_x)\n",
    "            total_loss += loss\n",
    "\n",
    "        # return avg loss\n",
    "        avg_loss = -total_loss/n\n",
    "        return avg_loss\n",
    "\n",
    "\n",
    "    def getSigmoidProbability(self, x):\n",
    "        '''\n",
    "        Compute softmax sigmoid probability vector of an example x belonging to each possible class.\n",
    "        @params:\n",
    "            x: a 1D Numpy array that is one example from the training set, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each possible class j, \n",
    "            where the element at j represents the probability of x belonging to j.\n",
    "        '''\n",
    "\n",
    "        probs = []\n",
    "        for j in range(self.n_classes):\n",
    "            # binary logistical predictor to see if a given x is in j, using the weights of class j\n",
    "            # outputs continuous float from [0,1]\n",
    "            # do this for each possible class, then pick the one with highest probablity (softmax) as the verdict class for this x\n",
    "            h_w_x = 1/(1+ np.exp(-self.weights[j].dot(x))) \n",
    "            probs.append(h_w_x)\n",
    "        \n",
    "        normalized_prob = softmax(probs)\n",
    "        return normalized_prob\n",
    "        \n",
    "    def getSoftmaxProbability(self, x):\n",
    "        '''\n",
    "        Compute softmax regular probability vector of an example belonging to each possible class.\n",
    "        @params:\n",
    "            x: a 1D Numpy array that is one example from the training set, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each possible class j, \n",
    "            where the element at j represents the probability of x belonging to j.\n",
    "        '''\n",
    "        return softmax(np.dot(self.weights, x))\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned weigths and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        # use one vs all algorithm for returning class with highest probablity of that x belonging to it\n",
    "        # let j be each possible class\n",
    "\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            normalized_prob = self.getSoftmaxProbability(x)\n",
    "            # print(\"norm prob:\", normalized_prob.shape)\n",
    "            # the index of highest probablity is used as the predicted class\n",
    "            x_prediction = np.argmax(normalized_prob)\n",
    "            predictions.append(x_prediction)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def accuracy(self, X, Y):\n",
    "        '''\n",
    "        Outputs the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "\n",
    "        # count how many 0s (accurately predicted examples) from the difference\n",
    "        predict = self.predict(X)\n",
    "        diff = predict - Y\n",
    "        num_correct = np.sum(diff == 0)\n",
    "\n",
    "        accuracy = num_correct/len(Y)\n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One vs. All Logistic Regression multi-class regression classifier ###\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "class OneVsAll:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_classes: int,\n",
    "        batch_size: int,\n",
    "        conv_threshold: float\n",
    "    ) -> None:\n",
    "        self.n_classes = n_classes\n",
    "        self.models = [\n",
    "            LogisticRegression(n_features, 2, batch_size, conv_threshold) \n",
    "            for _ in range(n_classes)\n",
    "        ]\n",
    "\n",
    "    def train(self, X: np.ndarray, Y: np.ndarray) -> None:\n",
    "        # Train a binary classifier for each class against all others\n",
    "        for class_label in range(self.n_classes):\n",
    "            binary_labels = (Y == class_label).astype(int)\n",
    "            self.models[class_label].train(X, binary_labels)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Get the probabilities of each class for each data point\n",
    "        # print(\"X\", X.shape)\n",
    "        probabilities = np.array([\n",
    "            model.predict(X) for model in self.models\n",
    "        ])\n",
    "\n",
    "        # Return the class with the highest probability for each data point\n",
    "        return np.argmax(probabilities, axis=0)\n",
    "\n",
    "    def accuracy(self, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All Pairs Logistic Regression multi-class regression classifier ###\n",
    "from typing import Any\n",
    "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
    "\n",
    "class AllPairs:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_classes: int,\n",
    "        batch_size: int,\n",
    "        conv_threshold: float,\n",
    "        classifier: str = \"own\",\n",
    "    ):\n",
    "        self.n_classes = n_classes\n",
    "        self.pairs = list(it.combinations(range(n_classes), 2))\n",
    "        print(self.pairs)\n",
    "        if classifier == \"own\":\n",
    "            self.models = {\n",
    "                (i, j): LogisticRegression(\n",
    "                    n_features,\n",
    "                    2,\n",
    "                    batch_size,\n",
    "                    conv_threshold\n",
    "                )\n",
    "                for i, j in self.pairs\n",
    "            }\n",
    "        else:\n",
    "            self.models = {\n",
    "                (i, j): SKLogisticRegression(\n",
    "                    fit_intercept=False\n",
    "                )\n",
    "                for i, j in self.pairs\n",
    "            }\n",
    "\n",
    "    def train(self, X: np.ndarray, Y: np.ndarray) -> None:\n",
    "        # Iterate over all pair combinations\n",
    "        print(\"self pairs:\", self.pairs)\n",
    "        for i, j in self.pairs:\n",
    "            print(\"ij\", i,j)\n",
    "            indices = (Y == i) | (Y == j)\n",
    "\n",
    "            # Get appropriate data\n",
    "            X_pair, Y_pair = X[indices], Y[indices]\n",
    "            Y_pair = (Y_pair == j).astype(int)\n",
    "\n",
    "            # Train on the pair\n",
    "            self.models[(i, j)].fit(X_pair, Y_pair)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Count predictions for each class\n",
    "        votes = np.zeros((len(X), self.n_classes))\n",
    "        for (i, j), model in self.models.items():\n",
    "            predictions = np.array(model.predict(X))\n",
    "            # print(\"predictions shape:\", predictions.shape, predictions)\n",
    "            votes[:, j] += predictions\n",
    "            votes[:, i] += (1 - predictions)\n",
    "\n",
    "        # Return the class with the most predictions\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "    def accuracy(self, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unit tests for AllPairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLearn weights:\n",
      "(LogisticRegression(fit_intercept=False), LogisticRegression(fit_intercept=False), LogisticRegression(fit_intercept=False))\n",
      "Class 0 weights [[-0.17315305 -0.51387022  0.16819876  0.09515732]]\n",
      "\n",
      "Class 1 weights [[ 0.24780742 -0.31339288 -0.40551825  0.21331106]]\n",
      "\n",
      "Class 2 weights [[ 0.38846112  0.22371409 -0.54569867  0.1243528 ]]\n",
      "\n",
      "[(0, 1), (0, 2), (1, 2)]\n",
      "self pairs: [(0, 1), (0, 2), (1, 2)]\n",
      "ij 0 1\n",
      "ij 0 2\n",
      "ij 1 2\n",
      "Our weights:\n",
      "Class (0, 1) weights [[-0.17315305 -0.51387022  0.16819876  0.09515732]]\n",
      "\n",
      "Class (0, 2) weights [[ 0.24780742 -0.31339288 -0.40551825  0.21331106]]\n",
      "\n",
      "Class (1, 2) weights [[ 0.38846112  0.22371409 -0.54569867  0.1243528 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SKLearn's one verse all logistic regression using the example X_1 and X_2 datasets.\n",
    "# Get the weight matrix after training for each of the sub binary classifiers to\n",
    "# compare against the weights of our own implementation of one verse all\n",
    "# logistic regression. Also compare predictions and accuracy.\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Smaller test data\n",
    "X_1 = np.random.rand(6, 3)\n",
    "y_1 = np.array([0, 1, 2])\n",
    "X_test_1 = np.random.rand(3, 3)\n",
    "\n",
    "# Larger test data\n",
    "X_2 = np.random.rand(8, 4)\n",
    "y_2 = np.array([0, 1, 2, 0, 1, 2, 0, 2])\n",
    "X_test_2 = np.random.rand(4, 4)\n",
    "\n",
    "# Train sklearn all pairs logistic regression\n",
    "sk_model = OneVsOneClassifier(SKLogisticRegression(fit_intercept=False))\n",
    "sk_model.fit(X_2, y_2)\n",
    "\n",
    "# Print weights of each classes binary classification model\n",
    "print(\"SKLearn weights:\")\n",
    "print(sk_model.estimators_)\n",
    "for i, model in enumerate(sk_model.estimators_):\n",
    "    print(f\"Class {i} weights {model.coef_}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Train our own all pairs logistic regression\n",
    "all_pairs = AllPairs(4, 3, 1, 1e-4, \"sk\")\n",
    "all_pairs.train(X_2, y_2)\n",
    "\n",
    "# Print weights of each classes binary classification model\n",
    "print(\"Our weights:\")\n",
    "for i, model in all_pairs.models.items():\n",
    "    print(f\"Class {i} weights {model.coef_}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "self pairs: [(0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n",
      "ij 0 1\n",
      "ij 0 2\n",
      "ij 0 3\n",
      "ij 0 4\n",
      "ij 1 2\n",
      "ij 1 3\n",
      "ij 1 4\n",
      "ij 2 3\n",
      "ij 2 4\n",
      "ij 3 4\n",
      "All-Pairs Accuracy: 0.276\n"
     ]
    }
   ],
   "source": [
    "### Dummy test ###\n",
    "n_samples = 500\n",
    "n_features = 10\n",
    "n_classes = 5\n",
    "batch_size = 100\n",
    "X = np.random.rand(n_samples, n_features)\n",
    "Y = np.random.randint(0, n_classes, n_samples)\n",
    "\n",
    "# #One-vs-All\n",
    "# ova = OneVsAll(n_features, n_classes, batch_size=32, conv_threshold=1e-4)\n",
    "# ova.train(X, Y)\n",
    "# print(f\"One-vs-All Accuracy: {ova.accuracy(X, Y)}\")\n",
    "\n",
    "# All-Pairs\n",
    "# all_pairs = AllPairs(n_features, n_classes, batch_size=32, conv_threshold=1e-4)\n",
    "# all_pairs.train(X, Y)\n",
    "# print(f\"All-Pairs Accuracy: {all_pairs.accuracy(X, Y)}\")\n",
    "\n",
    "# Train our own all pairs logistic regression\n",
    "all_pairs = AllPairs(n_features, n_classes, batch_size, 1e-4, \"sk\")\n",
    "all_pairs.train(X, Y)\n",
    "print(f\"All-Pairs Accuracy: {all_pairs.accuracy(X, Y)}\")\n",
    "# TODO: Add unit tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## German Numerical Credit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/german_numerical-binsensitive.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val, y_train, y_val, x_sex, x_age, x_sex_age\n\u001b[1;32m     51\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m X_train, X_test, y_train, y_test, x_sex, x_age, x_sex_age \u001b[38;5;241m=\u001b[39m \u001b[43mget_credit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train)\n\u001b[1;32m     54\u001b[0m n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(X_train[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m, in \u001b[0;36mget_credit\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_credit\u001b[39m():\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Gets and preprocesses German Credit data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/german_numerical-binsensitive.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Reads file - may change\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# MONTH categorizing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcut(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;241m3\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth_3\u001b[39m\u001b[38;5;124m'\u001b[39m], retbins\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/data2060/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/envs/data2060/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/german_numerical-binsensitive.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_credit():\n",
    "    \"\"\"\n",
    "    Gets and preprocesses German Credit data\n",
    "    \"\"\"\n",
    "    data = pd.read_csv('./data/german_numerical-binsensitive.csv') # Reads file - may change\n",
    "\n",
    "    # MONTH categorizing\n",
    "    data['month'] = pd.cut(data['month'],3, labels=['month_1', 'month_2', 'month_3'], retbins=True)[0]\n",
    "    # month bins: [ 3.932     , 26.66666667, 49.33333333, 72.        ]\n",
    "    a = pd.get_dummies(data['month'])\n",
    "    data = pd.concat([data, a], axis = 1)\n",
    "    data = data.drop(['month'], axis=1)\n",
    "\n",
    "    # CREDIT categorizing\n",
    "    data['credit_amount'] = pd.cut(data['credit_amount'], 3, labels=['cred_amt_1', 'cred_amt_2', 'cred_amt_3'], retbins=True)[0]\n",
    "    # credit bins: [  231.826,  6308.   , 12366.   , 18424.   ]\n",
    "    a = pd.get_dummies(data['credit_amount'])\n",
    "    data = pd.concat([data, a], axis = 1)\n",
    "    data = data.drop(['credit_amount'], axis=1)\n",
    "\n",
    "    for header in ['investment_as_income_percentage', 'residence_since', 'number_of_credits']:\n",
    "        a = pd.get_dummies(data[header], prefix=header)\n",
    "        data = pd.concat([data, a], axis = 1)\n",
    "        data = data.drop([header], axis=1)\n",
    "\n",
    "    # change from 1-2 classes to 0-1 classes\n",
    "    data['people_liable_for'] = data['people_liable_for'] -1\n",
    "    data['credit'] = -1*(data['credit']) + 2 # original encoding 1: good, 2: bad. we switch to 1: good, 0: bad\n",
    "\n",
    "    # balance dataset\n",
    "    data = data.reindex(np.random.permutation(data.index)) # shuffle\n",
    "    pos = data.loc[data['credit'] == 1]\n",
    "    neg = data.loc[data['credit'] == 0][:350]\n",
    "    combined = pd.concat([pos, neg])\n",
    "\n",
    "    y = data.iloc[:, data.columns == 'credit'].to_numpy()\n",
    "    x = data.drop(['credit', 'sex', 'age', 'sex-age'], axis=1).to_numpy()\n",
    "\n",
    "    # split into train and validation\n",
    "    X_train, X_val, y_train, y_val = x[:350, :], x[351:526, :], y[:350, :].reshape([350,]), y[351:526, :].reshape([175,])\n",
    "\n",
    "    # keep info about sex and age of validation rows for fairness portion\n",
    "    x_sex = data.iloc[:, data.columns == 'sex'].to_numpy()[351:526].reshape([175,])\n",
    "    x_age = data.iloc[:, data.columns == 'age'].to_numpy()[351:526].reshape([175,])\n",
    "    x_sex_age = data.iloc[:, data.columns == 'sex-age'].to_numpy()[351:526].reshape([175,])\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, x_sex, x_age, x_sex_age\n",
    "\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test, x_sex, x_age, x_sex_age = get_credit()\n",
    "n_samples = len(X_train)\n",
    "n_features = len(X_train[0])\n",
    "n_classes = 2\n",
    "\n",
    "# process data\n",
    "X_train[X_train == False] = 0.0\n",
    "X_train[X_train == True] = 1.0\n",
    "X_test[X_test == False] = 0.0\n",
    "X_test[X_test == True] = 1.0\n",
    "\n",
    "train_bias = np.ones((n_samples, 1))\n",
    "X_train = np.hstack((X_train, train_bias))\n",
    "\n",
    "test_bias = np.ones((len(X_test), 1))\n",
    "X_test = np.hstack((X_test, test_bias))\n",
    "\n",
    "print(\"x shape:\", X_train.shape, X_test.shape)\n",
    "print(\"num features:\", n_features)\n",
    "\n",
    "batch_size = 50\n",
    "conv_threshold = 1e-5\n",
    "\n",
    "# One-vs-All\n",
    "# ova = OneVsAll(n_features, n_classes, batch_size=batch_size, conv_threshold=conv_threshold)\n",
    "# ova.train(X_train, y_train)\n",
    "# print(f\"One-vs-All Accuracy: {ova.accuracy(X_test, y_test)}\")\n",
    "\n",
    "# All-Pairs\n",
    "\n",
    "all_pairs = AllPairs(n_features, n_classes, batch_size=batch_size, conv_threshold=conv_threshold)\n",
    "all_pairs.train(X_train, y_train)\n",
    "print(f\"All-Pairs Accuracy: {all_pairs.accuracy(X_test, y_test)}\")\n",
    "\n",
    "all_pairs = AllPairs(n_features, n_classes, batch_size=batch_size, conv_threshold=conv_threshold, classifier='sk')\n",
    "all_pairs.train(X_train, y_train)\n",
    "print(f\"All-Pairs Accuracy: {all_pairs.accuracy(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SkLearn Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number samples:  150 Number Feats:  4\n",
      "[(0, 1), (0, 2), (1, 2)]\n",
      "self pairs: [(0, 1), (0, 2), (1, 2)]\n",
      "ij 0 1\n",
      "ij 0 2\n",
      "ij 1 2\n",
      "All-Pairs Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "Y = data.target\n",
    "\n",
    "n_samples = len(X)\n",
    "n_features = len(X[0])\n",
    "n_classes = 3\n",
    "batch_size = 10\n",
    "conv_threshold = 1e-6\n",
    "\n",
    "print(\"Number samples: \", n_samples, \"Number Feats: \", n_features)\n",
    "\n",
    "# shuffle data\n",
    "p = np.random.permutation(n_samples)\n",
    "X_ = X[p]\n",
    "Y_ = Y[p]\n",
    "\n",
    "# add bias to shuffled data\n",
    "bias = np.ones((n_samples,1))\n",
    "X_ = np.hstack((X_, bias))\n",
    "\n",
    "# split data 80/20\n",
    "train_size = int(.8*n_samples)\n",
    "X_train = X_[:train_size,:]\n",
    "y_train = Y_[:train_size]\n",
    "\n",
    "X_test = X_[train_size:, :]\n",
    "y_test = Y_[train_size:]\n",
    "\n",
    "\n",
    "# # One-vs-All\n",
    "# ova = OneVsAll(n_features, n_classes, batch_size=batch_size, conv_threshold=conv_threshold)\n",
    "# ova.train(X_train, y_train)\n",
    "# print(f\"One-vs-All Accuracy: {ova.accuracy(X_test, y_test)}\")\n",
    "\n",
    "# All-Pairs\n",
    "\n",
    "all_pairs = AllPairs(n_features, n_classes, batch_size=batch_size, conv_threshold=conv_threshold, classifier='sk')\n",
    "all_pairs.train(X_train, y_train)\n",
    "print(f\"All-Pairs Accuracy: {all_pairs.accuracy(X_test, y_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
